{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision Quick and Dirty - CSS Summer School :)\n",
    "## This notebook contains a set of quick scripts to extract interpretable image features. \n",
    "#### Features include color distribution, sharpness, faces, and objects.\n",
    "\n",
    "It is *almost* plug-and-play, you will just have to create a couple of API keys, install a few libraries, and you are done. Just take this code and put it in a loop over all your images to extract features for your whole dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Requirements\n",
    "* OPENCV: pip install opencv-python\n",
    "* Tensorflow: pip install tensorflow==1.14.0\n",
    "* Google Vision API: pip install google-cloud-vision\n",
    "* Numpy/Math/Matplotlib (probably you have them already)\n",
    "* All files/scripts in this folder\n",
    "##### API keys\n",
    "* Face++: https://console.faceplusplus.com It's free ;D\n",
    "* Google Vision API: create credentials from https://console.cloud.google.com/apis/credentials->create credentials-->service account keys, then save the resulting file into credentials/vision_api.json\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "importing the libraries\n",
    "'''\n",
    "#general\n",
    "import os\n",
    "import math\n",
    "import json\n",
    "\n",
    "#for matrix computations and plotting\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#import libraries for google Vision API\n",
    "from google.cloud import vision\n",
    "\n",
    "#importing scripts for Face++\n",
    "from facepp import API\n",
    "from facepp import File\n",
    "\n",
    "#you will have to have tensorflow installed to run the code in the file below. \n",
    "import classify_image_tensorflow as tfclassify\n",
    "\n",
    "'''\n",
    "initializing context\n",
    "'''\n",
    "#replace this with your API keys for Face++\n",
    "API_KEY = ''\n",
    "API_SECRET = ''\n",
    "api = API(API_KEY, API_SECRET)\n",
    "\n",
    "#this is the environment variable with the credentials for Google Vision API\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"credentials/vision_api.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "A couple of useful plotting functions - let's skip the details, this is just for us to visulize the images we are dealing with\n",
    "'''\n",
    "\n",
    "def plot_image(img):\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) #transforming to rgb for visualization\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(10.5, 10.5)\n",
    "    plt.imshow(img_rgb)\n",
    "    plt.xticks([]), plt.yticks([])  # to hide tick values on X and Y axis\n",
    "    plt.show()\n",
    "\n",
    "def plot_hsv(h,s,v):\n",
    "    fig, (ax1, ax2,ax3) = plt.subplots(1, 3)\n",
    "    fig.set_size_inches(18.5, 4)\n",
    "\n",
    "    fig.suptitle('RGB IMAGE')\n",
    "    ax1.imshow(h,cmap=\"hsv\")\n",
    "    ax1.set_title('Hue')\n",
    "    ax2.imshow(s,cmap=\"gray\")\n",
    "    ax2.set_title('Saturation')\n",
    "    ax3.imshow(v,cmap=\"gray\")\n",
    "    ax3.set_title('Brightness')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Colors\n",
    "###  Goal: extract a color distribution in a human-readable format\n",
    "Starting point: \n",
    "OPENCV, the simplest library for image analysis. By default, OpenCV reads the image in the RGB space.\n",
    "![Image](https://img1.freepng.es/20180625/gpx/kisspng-rgb-color-space-rgb-color-model-light-5b30931fa1fb14.2738796215299100476635.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "here we read and plot an image, and try to understand what the image is made of\n",
    "'''\n",
    "img = cv2.imread('images/Fei-Fei.jpg')\n",
    "plot_image(img)\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "the image is 3 dimensional matrix, the first 2 dimensions are width and height, the third is the depth, or color channel. \n",
    "For color images, the depth is 3, Red, Green, and Blue.\n",
    "'''\n",
    "img[:4,:4,:].T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampling RGB histograms is not very effective if we want to get interpretable features.\n",
    "__The HSV Space gives a more interpretable representation.__\n",
    "<img align=\"left\" width=\"300\" height=\"300\" src=\"https://www.mathworks.com/help/images/hsvcone.gif\">\n",
    "<img align=\"center\" width=\"300\" height=\"300\" src=\"http://www.texample.net/media/tikz/examples/PNG/hsv-shading.png\">\n",
    "The Hue value represent the pure color tone, it's a value from 0 to 360 (180 in OpenCV) where 0 is red, 30 is orange, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "let's have a look at what the hsv space looks like\n",
    "'''\n",
    "hsv_img = cv2.cvtColor(img, cv2.COLOR_BGR2HSV) #convert irgbto hsv\n",
    "h,s,v = cv2.split(hsv_img) #isolating the 3 channels\n",
    "plot_hsv(h,s,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "What are the dominant colors of an image? \n",
    "Just count the number of pixels in the hue space belonging to each color.\n",
    "'''\n",
    "def extract_color_distribution(h):\n",
    "    #creating a dictionary of 12 colors sampled from the hue space\n",
    "    color_names=['Red','Orange','Yellow','Yellow-Green','Green','Aqua',\n",
    "                 'Cyan','Azure','Blue','Violet','Magenta,','Rose'] \n",
    "    #quantize each pixel from a value in the range (180) to a value between 0 and 11\n",
    "    h_quant=np.floor(np.divide(h,15)) \n",
    "    #compute distribution over these 12 values\n",
    "    color_values=np.histogram(h_quant,12)[0]\n",
    "    color_values=color_values/float(h.shape[0]*h.shape[1])\n",
    "    #assign a label to each bin of the color distribution\n",
    "    color_dict={color_names[i]:color_values[i] for i in range(len(color_values))}\n",
    "    return color_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Extracting color dictionary from the hue channel of the original image\n",
    "'''\n",
    "color_dict=extract_color_distribution(h)\n",
    "color_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Just for fun - changing hue values\n",
    "'''\n",
    "h_new=np.add(h,40)\n",
    "hsv_img_new=hsv_img.copy()\n",
    "hsv_img_new[:,:,0]=h_new\n",
    "img_converted= cv2.cvtColor(hsv_img_new, cv2.COLOR_HSV2BGR)\n",
    "plot_image(img_converted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_holi = cv2.imread('images/holi.jpg')\n",
    "hsv_img_holi = cv2.cvtColor(img_holi, cv2.COLOR_BGR2HSV) #convert it to hsv\n",
    "h_holi,s_holi,v_holi = cv2.split(hsv_img_holi)\n",
    "plot_image(img_holi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#brightness is also a good feature to distinguish between different images\n",
    "[np.mean(v),  np.mean(v_holi)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[np.mean(s),  np.mean(s_holi)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Computing Sharpness\n",
    "#### A good proxy for Image Quality\n",
    "Sharpness= definition of image edges, clarity of the shapes\n",
    "\n",
    "\n",
    "<img align=\"left\" width=\"600\" height=\"300\" src=\"https://improvephotography.com/wp-content/uploads/2014/07/sharpness-eyes-example.jpg\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "computing image sharpness as the magnitude of the gradient computed through Sobel edge detectors\n",
    "'''\n",
    "def compute_sharpness(img):\n",
    "    img_gray=cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    dx = cv2.Sobel(img_gray,cv2.CV_64F,1,0,ksize=5)\n",
    "    dy = cv2.Sobel(img_gray,cv2.CV_64F,0,1,ksize=5)\n",
    "    return dx,dy\n",
    "\n",
    "def plot_sharpness(img,dx,dy):\n",
    "    plt.clf\n",
    "    img_rgb=cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img_gray=cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    dx = cv2.Sobel(img_gray,cv2.CV_64F,1,0,ksize=31)\n",
    "    dy = cv2.Sobel(img_gray,cv2.CV_64F,0,1,ksize=31)\n",
    "    fig, (ax1, ax2,ax3) = plt.subplots(1,3)\n",
    "    fig.set_size_inches(20, 5)\n",
    "    fig.suptitle('Edge Detection with Sobel Filter')\n",
    "    ax1.imshow(img_rgb)\n",
    "    ax1.set_title('Original')\n",
    "    ax2.imshow(dx,cmap=\"gray\")\n",
    "    ax2.set_title('Vertical edges')\n",
    "    ax3.imshow(dy,cmap=\"gray\")\n",
    "    ax3.set_title(\"Horizontal edges\")\n",
    "    plt.show()\n",
    "\n",
    "def sharpness_feature(dx,dy):\n",
    "    gradient_magnitude=cv2.magnitude(dx,dy)\n",
    "    return np.sum(gradient_magnitude)/float(dx.shape[0]*dx.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#computing sharpness on Fei Fei's image\n",
    "dx,dy=compute_sharpness(img)\n",
    "plot_sharpness(img,dx,dy)\n",
    "sharpness=sharpness_feature(dx,dy)\n",
    "sharpness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Testing that the feature works by looking at the two halves of the image\n",
    "'''\n",
    "left_half=img[:,:int(img.shape[1]/float(2)),:]\n",
    "right_half=img[:,int(img.shape[1]/float(2)):,:]\n",
    "\n",
    "dxl,dyl=compute_sharpness(left_half)\n",
    "dxr,dyr=compute_sharpness(right_half)\n",
    "plt.clf()\n",
    "fig, (ax1, ax2) = plt.subplots(1,2)\n",
    "fig.set_size_inches(20, 5)\n",
    "fig.suptitle('Sharpness feature')\n",
    "ax1.imshow(cv2.cvtColor(left_half, cv2.COLOR_BGR2RGB))\n",
    "ax1.set_title('Sharpness: '+str(sharpness_feature(dxl,dyl)))\n",
    "ax2.imshow(cv2.cvtColor(right_half, cv2.COLOR_BGR2RGB))\n",
    "ax2.set_title('Sharpness: '+str(sharpness_feature(dxr,dyr)))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Faces\n",
    "#### Face detection, gender, age, expression classification, and face landmark localization\n",
    "Why do we care about faces?\n",
    "![Image](images/engageus.png)\n",
    "There are a number of open source solutions available, based on tensroflow. Here we look at an existing API, Face++, that has been shown to be very accurate to detect certain properties.\n",
    "Due to the training data, some of the models behind these APIs are more or less accurate depending on the demographics of the faces to be characterized.\n",
    "![Image](images/gendershades.png)\n",
    "Tip: when you do large-scale analysis, always double check face model biases with the gendershades dataset: http://gendershades.org\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Calling Face++ Api. All files and libraries to generate this call as in this repo. \n",
    "Just make sure you import them, and nothing to worry about. \n",
    "The face detection function also allows for ethnicity detection. Skipping here for inclusiveness purpose.\n",
    "Face++ also has more functions beyond classifying faces. \n",
    "Just check the request URL (for example: https://console.faceplusplus.com/documents/10880589) \n",
    "and change it in the detect funciton of the faceapp.py file.\n",
    "'''\n",
    "facedata=api.detect(image_file =File('images/Fei-Fei.jpg'),return_attributes='gender,age,smiling,eyestatus');\n",
    "#this function returns a dictionary. The \"faces\" field  is a list of X elements where X is the number of faces detected\n",
    "len(facedata[\"faces\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#info about the first face detected\n",
    "facedata[\"faces\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#info about the second face detected\n",
    "facedata[\"faces\"][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting Objects\n",
    "#### Everything beyond faces\n",
    "There are a two ways of doing this. The open source way is to use tensorflow-based models trained on Imagenet: http://www.image-net.org/. \n",
    "Imagenet is the largest image dataset available, the visual version of imagenet, it spans thousands of categories. Generally it is used by CV researchers as a benchmark for image classifiers. The most popular versions of imagenet-based classifiers are able to distinguish between 1k objects.\n",
    "The complete list of objects is here: https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a\n",
    "This script uses the Inception-v3 Architecture\n",
    "![Image](https://4.bp.blogspot.com/-TMOLlkJBxms/Vt3HQXpE2cI/AAAAAAAAA8E/7X7XRFOY6Xo/s1600/image03.png)\n",
    "Tip: this code is using an older version of the inception model, to download the latest one, change the url in the classify_image_tensorflow.py script to: http://download.tensorflow.org/models/image/imagenet/inception-latest.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''Just one line of code. This returns the top-5 predictions, i.e. the 5 output neurons and their activation score,\n",
    "but let's look at the objects for which the model is most confident about.'''\n",
    "plot_image(img)\n",
    "tfclassify.classify('images/Fei-Fei.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Let's try with another image'''\n",
    "img_serena=cv2.imread('images/serena.jpg')\n",
    "plot_image(img_serena)\n",
    "tfclassify.classify('images/serena.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting absolutely everyting\n",
    "#### Using Google Vision API\n",
    "You have a limit of 1k requests with the free account. But Google Vision API detects face characteristics, object, scenes, and web entitites.\n",
    "Similar to other models, the training data plays an important role. Vision API somehow clssifies in different ways images about the same subject coming from different places in the world.\n",
    "![Image](images/bias.png)\n",
    "Improvements will come thanks to the recent efforts in the community including the \"Inclusive Image Challenge\" https://www.kaggle.com/c/inclusive-images-challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Just prepare the request for the API, using your image path. \n",
    "You can also use image urls if the images you want to classify are in the web.\n",
    "Then run the annotate_image function.\n",
    "'''\n",
    "req = {\n",
    "    \"image\": {\"source\": {'filename': 'images/serena.jpg'}}}\n",
    "client = vision.ImageAnnotatorClient()\n",
    "\n",
    "response = client.annotate_image(req)\n",
    "#let's look at the coolest part: the web entities detected\n",
    "response.web_detection.web_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.label_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.face_annotations[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The End!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
